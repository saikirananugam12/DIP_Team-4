{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVM5zTWQIylE",
        "outputId": "5c37cdd0-f6fe-4457-b777-c780a8d360bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    file  age  gender  race  service_test\n",
            "0  27327    1       1     2         False\n",
            "1  63584    0       0     1         False\n",
            "2  35860    6       0     4          True\n",
            "3  83767    4       1     2          True\n",
            "4  66866    1       0     3         False\n",
            "Number of samples: 8781\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to your cleaned and encoded CSV file\n",
        "csv_path = '/content/drive/MyDrive/Trainselect/fairface_filtered_8781.csv'\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# Inspect the dataset\n",
        "print(data.head())\n",
        "print(f\"Number of samples: {len(data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-dvV3YIJGRS",
        "outputId": "680c64aa-7235-48af-ff54-d4ee2391da04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 7024, Validation samples: 1757\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Extract file paths and labels\n",
        "file_paths = data['file'].values\n",
        "labels = data[['age', 'gender', 'race']].values\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "    file_paths, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_files)}, Validation samples: {len(val_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDq8mXsLJLpd",
        "outputId": "14c8084d-420b-4015-f471-8ceb519ae0f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                file  age  gender  race  \\\n",
            "0  /content/drive/MyDrive/Trainselect/Train selec...    1       1     2   \n",
            "1  /content/drive/MyDrive/Trainselect/Train selec...    0       0     1   \n",
            "2  /content/drive/MyDrive/Trainselect/Train selec...    6       0     4   \n",
            "3  /content/drive/MyDrive/Trainselect/Train selec...    4       1     2   \n",
            "4  /content/drive/MyDrive/Trainselect/Train selec...    1       0     3   \n",
            "\n",
            "   service_test  \n",
            "0         False  \n",
            "1         False  \n",
            "2          True  \n",
            "3          True  \n",
            "4         False  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_path = '/content/drive/MyDrive/Trainselect/fairface_filtered_8781.csv'\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# Define the folder containing the images\n",
        "images_folder = '/content/drive/MyDrive/Trainselect/Train select'\n",
        "\n",
        "# Create full file paths from the file numbers\n",
        "data['file'] = data['file'].apply(lambda x: os.path.join(images_folder, f\"{x}.jpg.jpg\"))\n",
        "\n",
        "# Verify the updated file paths\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYeDfBFwJWty",
        "outputId": "e20020b4-1307-4de0-8813-ec320a638e6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of valid file paths: 7686\n",
            "Example file paths: ['/content/drive/MyDrive/Trainselect/Train select/27327.jpg.jpg'\n",
            " '/content/drive/MyDrive/Trainselect/Train select/63584.jpg.jpg'\n",
            " '/content/drive/MyDrive/Trainselect/Train select/35860.jpg.jpg'\n",
            " '/content/drive/MyDrive/Trainselect/Train select/83767.jpg.jpg'\n",
            " '/content/drive/MyDrive/Trainselect/Train select/66866.jpg.jpg']\n"
          ]
        }
      ],
      "source": [
        "# Filter rows where the file exists\n",
        "data = data[data['file'].apply(os.path.exists)]\n",
        "\n",
        "# Extract file paths and labels\n",
        "file_paths = data['file'].values\n",
        "labels = data[['age', 'gender', 'race']].values  # Adjust to match your encoded columns\n",
        "\n",
        "# Verify the number of valid file paths\n",
        "print(f\"Number of valid file paths: {len(file_paths)}\")\n",
        "print(f\"Example file paths: {file_paths[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a new column for stratification by combining age, gender, and race\n",
        "data['stratify_label'] = (\n",
        "    data['age'].astype(str) + '_' +\n",
        "    data['gender'].astype(str) + '_' +\n",
        "    data['race'].astype(str)\n",
        ")\n",
        "\n",
        "# Perform stratified split\n",
        "train_data, val_data = train_test_split(\n",
        "    data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=data['stratify_label']  # Ensure balanced split\n",
        ")\n",
        "\n",
        "# Extract file paths and labels for training and validation\n",
        "train_files = train_data['file'].tolist()\n",
        "val_files = val_data['file'].tolist()\n",
        "train_labels = train_data[['age', 'gender', 'race']].to_dict('records')\n",
        "val_labels = val_data[['age', 'gender', 'race']].to_dict('records')\n",
        "\n",
        "# Confirm the splits\n",
        "print(f\"Training samples: {len(train_files)}\")\n",
        "print(f\"Validation samples: {len(val_files)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyFf_WPexCy_",
        "outputId": "63bf42b4-99c4-4928-9653-194054f7cbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 6148\n",
            "Validation samples: 1538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Preprocess labels: Convert list of dictionaries to separate arrays\n",
        "train_labels_age = [label['age'] for label in train_labels]\n",
        "train_labels_gender = [label['gender'] for label in train_labels]\n",
        "train_labels_race = [label['race'] for label in train_labels]\n",
        "\n",
        "val_labels_age = [label['age'] for label in val_labels]\n",
        "val_labels_gender = [label['gender'] for label in val_labels]\n",
        "val_labels_race = [label['race'] for label in val_labels]\n",
        "\n",
        "# Preprocessing function for images and labels\n",
        "def preprocess_image(file_path, age, gender, race):\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (224, 224))  # Resize for EfficientNetB0\n",
        "    img = img / 255.0  # Normalize to [0, 1]\n",
        "    return img, {'age': age, 'gender': gender, 'race': race}\n",
        "\n",
        "# Create training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_files, train_labels_age, train_labels_gender, train_labels_race)\n",
        ")\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda file_path, age, gender, race: preprocess_image(file_path, age, gender, race)\n",
        ").shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_files, val_labels_age, val_labels_gender, val_labels_race)\n",
        ")\n",
        "val_dataset = val_dataset.map(\n",
        "    lambda file_path, age, gender, race: preprocess_image(file_path, age, gender, race)\n",
        ").batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Verify the datasets\n",
        "for images, labels in train_dataset.take(1):\n",
        "    print(f\"Image batch shape: {images.shape}\")\n",
        "    print(f\"Age labels batch shape: {labels['age'].shape}\")\n",
        "    print(f\"Gender labels batch shape: {labels['gender'].shape}\")\n",
        "    print(f\"Race labels batch shape: {labels['race'].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gORVOeVuxYg3",
        "outputId": "d942a230-95b4-45e7-b6a3-02eec438261b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: (32, 224, 224, 3)\n",
            "Age labels batch shape: (32,)\n",
            "Gender labels batch shape: (32,)\n",
            "Race labels batch shape: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "oadoQu3ENC4r",
        "outputId": "901ad8de-3810-468e-b41c-d8b6ce357e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ inception_v3 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │     \u001b[38;5;34m21,802,784\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ inception_v3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ age (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m16,392\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gender (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │          \u001b[38;5;34m4,098\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ race (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m14,343\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ inception_v3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ inception_v3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ age (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,392</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ gender (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,098</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ race (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">14,343</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,837,617\u001b[0m (83.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,837,617</span> (83.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,833\u001b[0m (136.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,833</span> (136.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m21,802,784\u001b[0m (83.17 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> (83.17 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (224, 224, 3)  # Assuming images are resized to 224x224\n",
        "\n",
        "# Load the pre-trained InceptionV3 model\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom layers on top of the InceptionV3 model\n",
        "inputs = Input(shape=input_shape)\n",
        "x = base_model(inputs, training=False)  # Pass the inputs through the pre-trained base model\n",
        "x = GlobalAveragePooling2D()(x)  # Pool the features from the Inception model\n",
        "x = Dropout(0.5)(x)  # Add a dropout layer for regularization\n",
        "\n",
        "# Output layers\n",
        "age_output = Dense(8, activation='softmax', name='age')(x)  # Predict age with 8 classes\n",
        "gender_output = Dense(2, activation='softmax', name='gender')(x)  # Predict gender with 2 classes\n",
        "race_output = Dense(7, activation='softmax', name='race')(x)  # Predict race with 7 classes\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=inputs, outputs=[age_output, gender_output, race_output])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss={\n",
        "        'age': 'sparse_categorical_crossentropy',\n",
        "        'gender': 'sparse_categorical_crossentropy',\n",
        "        'race': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    metrics={\n",
        "        'age': 'accuracy',\n",
        "        'gender': 'accuracy',\n",
        "        'race': 'accuracy',\n",
        "    }\n",
        ")\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI6nT-AQPcgx"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower learning rate for transfer learning\n",
        "    loss={\n",
        "        'age': 'sparse_categorical_crossentropy',  # Multi-class classification for age\n",
        "        'gender': 'sparse_categorical_crossentropy',  # Binary classification for gender\n",
        "        'race': 'sparse_categorical_crossentropy',  # Multi-class classification for race\n",
        "    },\n",
        "    metrics={\n",
        "        'age': ['accuracy'],  # Track accuracy for age predictions\n",
        "        'gender': ['accuracy'],  # Track accuracy for gender predictions\n",
        "        'race': ['accuracy'],  # Track accuracy for race predictions\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoLlTCUSQmNV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath=\"inception_model_best.keras\",  # Save the best model during training\n",
        "        monitor=\"val_loss\",  # Monitor validation loss\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor=\"val_loss\",  # Stop training when validation loss stops improving\n",
        "        patience=3,  # Number of epochs to wait before stopping\n",
        "        verbose=1,\n",
        "        restore_best_weights=True  # Restore the best weights at the end\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\",  # Reduce learning rate when validation loss stops improving\n",
        "        factor=0.5,  # Reduce by half\n",
        "        patience=2,  # Wait for 2 epochs before reducing\n",
        "        verbose=1\n",
        "    )\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the MobileNetV2 model pre-trained on ImageNet, excluding the top layer\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom top layers for your specific task\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)  # Add a global average pooling layer\n",
        "x = Dense(256, activation='relu')(x)  # Add a fully connected layer\n",
        "outputs = {\n",
        "    'age': Dense(8, activation='softmax', name='age')(x),\n",
        "    'gender': Dense(2, activation='sigmoid', name='gender')(x),\n",
        "    'race': Dense(7, activation='softmax', name='race')(x),\n",
        "}\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Compile the model with the initial frozen layers\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),  # Initial higher learning rate\n",
        "    loss={\n",
        "        'age': 'sparse_categorical_crossentropy',\n",
        "        'gender': 'sparse_categorical_crossentropy',\n",
        "        'race': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    metrics={\n",
        "        'age': 'accuracy',\n",
        "        'gender': 'accuracy',\n",
        "        'race': 'accuracy',\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjRK2tEjrE9n",
        "outputId": "08691ab1-062b-4d9f-a8aa-c9eb2695587e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-fcb2f082e197>:7: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights='imagenet', include_top=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
        "import json\n",
        "\n",
        "# Custom callback to save epoch values\n",
        "class EpochLogger(Callback):\n",
        "    def __init__(self, log_file='epoch_log.json'):\n",
        "        self.log_file = log_file\n",
        "        self.epoch_data = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save epoch number and logs (loss, accuracy, etc.)\n",
        "        epoch_info = {'epoch': epoch + 1}\n",
        "        if logs:\n",
        "            epoch_info.update(logs)\n",
        "        self.epoch_data.append(epoch_info)\n",
        "\n",
        "        # Write to JSON file\n",
        "        with open(self.log_file, 'w') as file:\n",
        "            json.dump(self.epoch_data, file, indent=4)\n",
        "\n",
        "# Define checkpoint callback to save weights\n",
        "checkpoint_filepath = 'model_checkpoints/weights_epoch_{epoch:02d}.weights.h5'\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,  # Save only weights\n",
        "    monitor='val_loss',      # Monitor validation loss\n",
        "    mode='min',\n",
        "    save_best_only=False,    # Save weights after every epoch\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Load weights if available\n",
        "try:\n",
        "    latest_weights = tf.train.latest_checkpoint('model_checkpoints/')\n",
        "    if latest_weights:\n",
        "        model.load_weights(latest_weights)\n",
        "        print(f\"Loaded weights from {latest_weights}\")\n",
        "except Exception as e:\n",
        "    print(\"No weights found to load. Starting fresh.\")\n",
        "\n",
        "# Combine callbacks\n",
        "epoch_logger = EpochLogger(log_file='epoch_log.json')\n",
        "callbacks = [model_checkpoint, epoch_logger]\n"
      ],
      "metadata": {
        "id": "eTN1AeUo2O0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=20,  # Total desired epochs\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RfBgxCS3Qgi",
        "outputId": "40c41de1-8b6d-4ad3-9244-fd3f3ac35a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - age_accuracy: 0.2053 - age_loss: 2.0566 - gender_accuracy: 0.5696 - gender_loss: 0.6971 - loss: 4.7690 - race_accuracy: 0.1681 - race_loss: 2.0153\n",
            "Epoch 1: saving model to model_checkpoints/weights_epoch_01.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 344ms/step - age_accuracy: 0.2055 - age_loss: 2.0561 - gender_accuracy: 0.5698 - gender_loss: 0.6969 - loss: 4.7680 - race_accuracy: 0.1681 - race_loss: 2.0151 - val_age_accuracy: 0.2692 - val_age_loss: 1.8274 - val_gender_accuracy: 0.6749 - val_gender_loss: 0.5947 - val_loss: 4.3336 - val_race_accuracy: 0.2094 - val_race_loss: 1.8999\n",
            "Epoch 2/20\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - age_accuracy: 0.3056 - age_loss: 1.7561 - gender_accuracy: 0.6849 - gender_loss: 0.5926 - loss: 4.1908 - race_accuracy: 0.2452 - race_loss: 1.8421\n",
            "Epoch 2: saving model to model_checkpoints/weights_epoch_02.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 125ms/step - age_accuracy: 0.3056 - age_loss: 1.7560 - gender_accuracy: 0.6849 - gender_loss: 0.5926 - loss: 4.1906 - race_accuracy: 0.2452 - race_loss: 1.8420 - val_age_accuracy: 0.2926 - val_age_loss: 1.7445 - val_gender_accuracy: 0.6964 - val_gender_loss: 0.5745 - val_loss: 4.1905 - val_race_accuracy: 0.2497 - val_race_loss: 1.8620\n",
            "Epoch 3/20\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - age_accuracy: 0.3495 - age_loss: 1.6434 - gender_accuracy: 0.6963 - gender_loss: 0.5750 - loss: 3.9783 - race_accuracy: 0.3021 - race_loss: 1.7598\n",
            "Epoch 3: saving model to model_checkpoints/weights_epoch_03.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 120ms/step - age_accuracy: 0.3495 - age_loss: 1.6434 - gender_accuracy: 0.6964 - gender_loss: 0.5750 - loss: 3.9782 - race_accuracy: 0.3021 - race_loss: 1.7598 - val_age_accuracy: 0.3205 - val_age_loss: 1.7264 - val_gender_accuracy: 0.6899 - val_gender_loss: 0.5698 - val_loss: 4.1107 - val_race_accuracy: 0.2913 - val_race_loss: 1.8095\n",
            "Epoch 4/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - age_accuracy: 0.3851 - age_loss: 1.5818 - gender_accuracy: 0.7059 - gender_loss: 0.5526 - loss: 3.8284 - race_accuracy: 0.3450 - race_loss: 1.6939\n",
            "Epoch 4: saving model to model_checkpoints/weights_epoch_04.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 120ms/step - age_accuracy: 0.3850 - age_loss: 1.5818 - gender_accuracy: 0.7060 - gender_loss: 0.5525 - loss: 3.8282 - race_accuracy: 0.3450 - race_loss: 1.6939 - val_age_accuracy: 0.3270 - val_age_loss: 1.6864 - val_gender_accuracy: 0.7068 - val_gender_loss: 0.5810 - val_loss: 4.0839 - val_race_accuracy: 0.2945 - val_race_loss: 1.8090\n",
            "Epoch 5/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - age_accuracy: 0.4000 - age_loss: 1.5342 - gender_accuracy: 0.7290 - gender_loss: 0.5347 - loss: 3.7129 - race_accuracy: 0.3686 - race_loss: 1.6440\n",
            "Epoch 5: saving model to model_checkpoints/weights_epoch_05.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 118ms/step - age_accuracy: 0.3999 - age_loss: 1.5342 - gender_accuracy: 0.7291 - gender_loss: 0.5346 - loss: 3.7127 - race_accuracy: 0.3687 - race_loss: 1.6439 - val_age_accuracy: 0.3238 - val_age_loss: 1.6722 - val_gender_accuracy: 0.7035 - val_gender_loss: 0.5592 - val_loss: 4.0309 - val_race_accuracy: 0.2913 - val_race_loss: 1.7851\n",
            "Epoch 6/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - age_accuracy: 0.4252 - age_loss: 1.4866 - gender_accuracy: 0.7414 - gender_loss: 0.5111 - loss: 3.5947 - race_accuracy: 0.3975 - race_loss: 1.5970\n",
            "Epoch 6: saving model to model_checkpoints/weights_epoch_06.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 122ms/step - age_accuracy: 0.4251 - age_loss: 1.4866 - gender_accuracy: 0.7414 - gender_loss: 0.5112 - loss: 3.5946 - race_accuracy: 0.3976 - race_loss: 1.5968 - val_age_accuracy: 0.3212 - val_age_loss: 1.6522 - val_gender_accuracy: 0.7152 - val_gender_loss: 0.5593 - val_loss: 3.9870 - val_race_accuracy: 0.3114 - val_race_loss: 1.7633\n",
            "Epoch 7/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - age_accuracy: 0.4388 - age_loss: 1.4456 - gender_accuracy: 0.7457 - gender_loss: 0.5065 - loss: 3.5080 - race_accuracy: 0.4229 - race_loss: 1.5558\n",
            "Epoch 7: saving model to model_checkpoints/weights_epoch_07.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 149ms/step - age_accuracy: 0.4388 - age_loss: 1.4457 - gender_accuracy: 0.7458 - gender_loss: 0.5065 - loss: 3.5079 - race_accuracy: 0.4230 - race_loss: 1.5557 - val_age_accuracy: 0.3414 - val_age_loss: 1.6508 - val_gender_accuracy: 0.7100 - val_gender_loss: 0.5564 - val_loss: 3.9845 - val_race_accuracy: 0.3121 - val_race_loss: 1.7654\n",
            "Epoch 8/20\n",
            "\u001b[1m191/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - age_accuracy: 0.4638 - age_loss: 1.4184 - gender_accuracy: 0.7611 - gender_loss: 0.4913 - loss: 3.4340 - race_accuracy: 0.4353 - race_loss: 1.5243\n",
            "Epoch 8: saving model to model_checkpoints/weights_epoch_08.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 125ms/step - age_accuracy: 0.4637 - age_loss: 1.4185 - gender_accuracy: 0.7610 - gender_loss: 0.4913 - loss: 3.4337 - race_accuracy: 0.4355 - race_loss: 1.5240 - val_age_accuracy: 0.3362 - val_age_loss: 1.6400 - val_gender_accuracy: 0.7159 - val_gender_loss: 0.5534 - val_loss: 3.9557 - val_race_accuracy: 0.3153 - val_race_loss: 1.7498\n",
            "Epoch 9/20\n",
            "\u001b[1m191/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - age_accuracy: 0.4812 - age_loss: 1.3734 - gender_accuracy: 0.7683 - gender_loss: 0.4755 - loss: 3.3282 - race_accuracy: 0.4574 - race_loss: 1.4794\n",
            "Epoch 9: saving model to model_checkpoints/weights_epoch_09.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 122ms/step - age_accuracy: 0.4810 - age_loss: 1.3736 - gender_accuracy: 0.7683 - gender_loss: 0.4754 - loss: 3.3283 - race_accuracy: 0.4575 - race_loss: 1.4792 - val_age_accuracy: 0.3322 - val_age_loss: 1.6492 - val_gender_accuracy: 0.7165 - val_gender_loss: 0.5530 - val_loss: 3.9550 - val_race_accuracy: 0.3108 - val_race_loss: 1.7413\n",
            "Epoch 10/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - age_accuracy: 0.5103 - age_loss: 1.3515 - gender_accuracy: 0.7842 - gender_loss: 0.4623 - loss: 3.2735 - race_accuracy: 0.4709 - race_loss: 1.4598\n",
            "Epoch 10: saving model to model_checkpoints/weights_epoch_10.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 123ms/step - age_accuracy: 0.5101 - age_loss: 1.3515 - gender_accuracy: 0.7842 - gender_loss: 0.4622 - loss: 3.2733 - race_accuracy: 0.4710 - race_loss: 1.4595 - val_age_accuracy: 0.3296 - val_age_loss: 1.6462 - val_gender_accuracy: 0.7191 - val_gender_loss: 0.5534 - val_loss: 3.9481 - val_race_accuracy: 0.3179 - val_race_loss: 1.7326\n",
            "Epoch 11/20\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - age_accuracy: 0.5077 - age_loss: 1.3270 - gender_accuracy: 0.7924 - gender_loss: 0.4504 - loss: 3.1890 - race_accuracy: 0.4957 - race_loss: 1.4116\n",
            "Epoch 11: saving model to model_checkpoints/weights_epoch_11.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 126ms/step - age_accuracy: 0.5077 - age_loss: 1.3270 - gender_accuracy: 0.7924 - gender_loss: 0.4504 - loss: 3.1890 - race_accuracy: 0.4957 - race_loss: 1.4116 - val_age_accuracy: 0.3466 - val_age_loss: 1.6393 - val_gender_accuracy: 0.7094 - val_gender_loss: 0.5643 - val_loss: 3.9421 - val_race_accuracy: 0.3303 - val_race_loss: 1.7279\n",
            "Epoch 12/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - age_accuracy: 0.5334 - age_loss: 1.2874 - gender_accuracy: 0.7959 - gender_loss: 0.4425 - loss: 3.1206 - race_accuracy: 0.5074 - race_loss: 1.3907\n",
            "Epoch 12: saving model to model_checkpoints/weights_epoch_12.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 124ms/step - age_accuracy: 0.5334 - age_loss: 1.2875 - gender_accuracy: 0.7959 - gender_loss: 0.4424 - loss: 3.1204 - race_accuracy: 0.5075 - race_loss: 1.3905 - val_age_accuracy: 0.3472 - val_age_loss: 1.6272 - val_gender_accuracy: 0.7178 - val_gender_loss: 0.5575 - val_loss: 3.9418 - val_race_accuracy: 0.3179 - val_race_loss: 1.7392\n",
            "Epoch 13/20\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - age_accuracy: 0.5408 - age_loss: 1.2602 - gender_accuracy: 0.8115 - gender_loss: 0.4299 - loss: 3.0461 - race_accuracy: 0.5195 - race_loss: 1.3560\n",
            "Epoch 13: saving model to model_checkpoints/weights_epoch_13.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 126ms/step - age_accuracy: 0.5408 - age_loss: 1.2602 - gender_accuracy: 0.8115 - gender_loss: 0.4299 - loss: 3.0460 - race_accuracy: 0.5196 - race_loss: 1.3559 - val_age_accuracy: 0.3427 - val_age_loss: 1.6319 - val_gender_accuracy: 0.7133 - val_gender_loss: 0.5572 - val_loss: 3.9353 - val_race_accuracy: 0.3205 - val_race_loss: 1.7318\n",
            "Epoch 14/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - age_accuracy: 0.5615 - age_loss: 1.2292 - gender_accuracy: 0.8134 - gender_loss: 0.4197 - loss: 2.9719 - race_accuracy: 0.5386 - race_loss: 1.3230 \n",
            "Epoch 14: saving model to model_checkpoints/weights_epoch_14.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 127ms/step - age_accuracy: 0.5614 - age_loss: 1.2294 - gender_accuracy: 0.8134 - gender_loss: 0.4197 - loss: 2.9719 - race_accuracy: 0.5387 - race_loss: 1.3228 - val_age_accuracy: 0.3485 - val_age_loss: 1.6527 - val_gender_accuracy: 0.7061 - val_gender_loss: 0.5629 - val_loss: 3.9570 - val_race_accuracy: 0.3368 - val_race_loss: 1.7274\n",
            "Epoch 15/20\n",
            "\u001b[1m191/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - age_accuracy: 0.5780 - age_loss: 1.2209 - gender_accuracy: 0.8281 - gender_loss: 0.4057 - loss: 2.9262 - race_accuracy: 0.5438 - race_loss: 1.2997\n",
            "Epoch 15: saving model to model_checkpoints/weights_epoch_15.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 127ms/step - age_accuracy: 0.5779 - age_loss: 1.2208 - gender_accuracy: 0.8281 - gender_loss: 0.4057 - loss: 2.9260 - race_accuracy: 0.5440 - race_loss: 1.2995 - val_age_accuracy: 0.3505 - val_age_loss: 1.6371 - val_gender_accuracy: 0.7139 - val_gender_loss: 0.5606 - val_loss: 3.9495 - val_race_accuracy: 0.3244 - val_race_loss: 1.7391\n",
            "Epoch 16/20\n",
            "\u001b[1m191/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - age_accuracy: 0.5723 - age_loss: 1.1886 - gender_accuracy: 0.8308 - gender_loss: 0.3973 - loss: 2.8667 - race_accuracy: 0.5603 - race_loss: 1.2808 \n",
            "Epoch 16: saving model to model_checkpoints/weights_epoch_16.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 126ms/step - age_accuracy: 0.5723 - age_loss: 1.1886 - gender_accuracy: 0.8308 - gender_loss: 0.3973 - loss: 2.8663 - race_accuracy: 0.5605 - race_loss: 1.2805 - val_age_accuracy: 0.3498 - val_age_loss: 1.6549 - val_gender_accuracy: 0.7087 - val_gender_loss: 0.5637 - val_loss: 3.9579 - val_race_accuracy: 0.3303 - val_race_loss: 1.7270\n",
            "Epoch 17/20\n",
            "\u001b[1m191/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - age_accuracy: 0.5885 - age_loss: 1.1635 - gender_accuracy: 0.8387 - gender_loss: 0.3837 - loss: 2.7752 - race_accuracy: 0.5822 - race_loss: 1.2280\n",
            "Epoch 17: saving model to model_checkpoints/weights_epoch_17.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 126ms/step - age_accuracy: 0.5886 - age_loss: 1.1635 - gender_accuracy: 0.8386 - gender_loss: 0.3837 - loss: 2.7753 - race_accuracy: 0.5823 - race_loss: 1.2280 - val_age_accuracy: 0.3589 - val_age_loss: 1.6363 - val_gender_accuracy: 0.7217 - val_gender_loss: 0.5706 - val_loss: 3.9504 - val_race_accuracy: 0.3349 - val_race_loss: 1.7231\n",
            "Epoch 18/20\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - age_accuracy: 0.6081 - age_loss: 1.1464 - gender_accuracy: 0.8489 - gender_loss: 0.3757 - loss: 2.7261 - race_accuracy: 0.5952 - race_loss: 1.2039 \n",
            "Epoch 18: saving model to model_checkpoints/weights_epoch_18.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 122ms/step - age_accuracy: 0.6081 - age_loss: 1.1464 - gender_accuracy: 0.8488 - gender_loss: 0.3757 - loss: 2.7261 - race_accuracy: 0.5952 - race_loss: 1.2039 - val_age_accuracy: 0.3518 - val_age_loss: 1.6469 - val_gender_accuracy: 0.7120 - val_gender_loss: 0.5738 - val_loss: 3.9733 - val_race_accuracy: 0.3160 - val_race_loss: 1.7312\n",
            "Epoch 19/20\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - age_accuracy: 0.6263 - age_loss: 1.1099 - gender_accuracy: 0.8555 - gender_loss: 0.3629 - loss: 2.6658 - race_accuracy: 0.5988 - race_loss: 1.1930\n",
            "Epoch 19: saving model to model_checkpoints/weights_epoch_19.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 121ms/step - age_accuracy: 0.6263 - age_loss: 1.1099 - gender_accuracy: 0.8555 - gender_loss: 0.3629 - loss: 2.6657 - race_accuracy: 0.5989 - race_loss: 1.1929 - val_age_accuracy: 0.3485 - val_age_loss: 1.6655 - val_gender_accuracy: 0.7165 - val_gender_loss: 0.5730 - val_loss: 4.0196 - val_race_accuracy: 0.3257 - val_race_loss: 1.7610\n",
            "Epoch 20/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - age_accuracy: 0.6298 - age_loss: 1.0943 - gender_accuracy: 0.8540 - gender_loss: 0.3605 - loss: 2.6209 - race_accuracy: 0.6110 - race_loss: 1.1660\n",
            "Epoch 20: saving model to model_checkpoints/weights_epoch_20.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 120ms/step - age_accuracy: 0.6299 - age_loss: 1.0943 - gender_accuracy: 0.8540 - gender_loss: 0.3605 - loss: 2.6207 - race_accuracy: 0.6111 - race_loss: 1.1659 - val_age_accuracy: 0.3524 - val_age_loss: 1.6540 - val_gender_accuracy: 0.7113 - val_gender_loss: 0.5766 - val_loss: 3.9948 - val_race_accuracy: 0.3140 - val_race_loss: 1.7489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the base model\n",
        "base_model.trainable = True  # Unfreeze the base model\n",
        "\n",
        "# Recompile the model with a lower learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss={\n",
        "        'age': 'sparse_categorical_crossentropy',\n",
        "        'gender': 'sparse_categorical_crossentropy',\n",
        "        'race': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    metrics={\n",
        "        'age': 'accuracy',\n",
        "        'gender': 'accuracy',\n",
        "        'race': 'accuracy',\n",
        "    }\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=20,  # Total desired epochs\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt-zVIWm8UH1",
        "outputId": "46873717-2658-4f4b-f410-07b89dd35b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - age_accuracy: 0.2410 - age_loss: 2.0329 - gender_accuracy: 0.5934 - gender_loss: 0.7621 - loss: 4.8595 - race_accuracy: 0.2377 - race_loss: 2.0644\n",
            "Epoch 1: saving model to model_checkpoints/weights_epoch_01.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 312ms/step - age_accuracy: 0.2411 - age_loss: 2.0325 - gender_accuracy: 0.5934 - gender_loss: 0.7620 - loss: 4.8585 - race_accuracy: 0.2377 - race_loss: 2.0639 - val_age_accuracy: 0.2692 - val_age_loss: 2.2054 - val_gender_accuracy: 0.6710 - val_gender_loss: 0.6400 - val_loss: 4.7416 - val_race_accuracy: 0.2880 - val_race_loss: 1.8834\n",
            "Epoch 2/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - age_accuracy: 0.3256 - age_loss: 1.7476 - gender_accuracy: 0.6579 - gender_loss: 0.6355 - loss: 4.1573 - race_accuracy: 0.3207 - race_loss: 1.7743\n",
            "Epoch 2: saving model to model_checkpoints/weights_epoch_02.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 150ms/step - age_accuracy: 0.3256 - age_loss: 1.7475 - gender_accuracy: 0.6579 - gender_loss: 0.6354 - loss: 4.1569 - race_accuracy: 0.3207 - race_loss: 1.7741 - val_age_accuracy: 0.2380 - val_age_loss: 2.4130 - val_gender_accuracy: 0.6450 - val_gender_loss: 0.6695 - val_loss: 5.1107 - val_race_accuracy: 0.2594 - val_race_loss: 2.0179\n",
            "Epoch 3/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - age_accuracy: 0.3818 - age_loss: 1.6205 - gender_accuracy: 0.6998 - gender_loss: 0.5742 - loss: 3.8514 - race_accuracy: 0.3583 - race_loss: 1.6567\n",
            "Epoch 3: saving model to model_checkpoints/weights_epoch_03.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 156ms/step - age_accuracy: 0.3817 - age_loss: 1.6204 - gender_accuracy: 0.6998 - gender_loss: 0.5742 - loss: 3.8510 - race_accuracy: 0.3584 - race_loss: 1.6565 - val_age_accuracy: 0.2315 - val_age_loss: 2.4388 - val_gender_accuracy: 0.6333 - val_gender_loss: 0.7086 - val_loss: 5.2142 - val_race_accuracy: 0.2555 - val_race_loss: 2.0800\n",
            "Epoch 4/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - age_accuracy: 0.4187 - age_loss: 1.5085 - gender_accuracy: 0.7518 - gender_loss: 0.5150 - loss: 3.5807 - race_accuracy: 0.3909 - race_loss: 1.5572\n",
            "Epoch 4: saving model to model_checkpoints/weights_epoch_04.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 159ms/step - age_accuracy: 0.4186 - age_loss: 1.5086 - gender_accuracy: 0.7517 - gender_loss: 0.5150 - loss: 3.5808 - race_accuracy: 0.3909 - race_loss: 1.5572 - val_age_accuracy: 0.2360 - val_age_loss: 2.3967 - val_gender_accuracy: 0.6216 - val_gender_loss: 0.7521 - val_loss: 5.2249 - val_race_accuracy: 0.2549 - val_race_loss: 2.1080\n",
            "Epoch 5/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - age_accuracy: 0.4364 - age_loss: 1.4360 - gender_accuracy: 0.7582 - gender_loss: 0.4990 - loss: 3.4183 - race_accuracy: 0.4329 - race_loss: 1.4833\n",
            "Epoch 5: saving model to model_checkpoints/weights_epoch_05.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 159ms/step - age_accuracy: 0.4364 - age_loss: 1.4361 - gender_accuracy: 0.7582 - gender_loss: 0.4989 - loss: 3.4181 - race_accuracy: 0.4329 - race_loss: 1.4831 - val_age_accuracy: 0.2243 - val_age_loss: 2.3374 - val_gender_accuracy: 0.6222 - val_gender_loss: 0.7604 - val_loss: 5.1807 - val_race_accuracy: 0.2568 - val_race_loss: 2.1291\n",
            "Epoch 6/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - age_accuracy: 0.4538 - age_loss: 1.3887 - gender_accuracy: 0.7856 - gender_loss: 0.4513 - loss: 3.2487 - race_accuracy: 0.4653 - race_loss: 1.4087\n",
            "Epoch 6: saving model to model_checkpoints/weights_epoch_06.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 163ms/step - age_accuracy: 0.4539 - age_loss: 1.3887 - gender_accuracy: 0.7857 - gender_loss: 0.4513 - loss: 3.2484 - race_accuracy: 0.4655 - race_loss: 1.4085 - val_age_accuracy: 0.2308 - val_age_loss: 2.2523 - val_gender_accuracy: 0.6190 - val_gender_loss: 0.7816 - val_loss: 5.1381 - val_race_accuracy: 0.2620 - val_race_loss: 2.1540\n",
            "Epoch 7/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - age_accuracy: 0.4991 - age_loss: 1.2985 - gender_accuracy: 0.8036 - gender_loss: 0.4289 - loss: 3.0697 - race_accuracy: 0.5142 - race_loss: 1.3423\n",
            "Epoch 7: saving model to model_checkpoints/weights_epoch_07.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 157ms/step - age_accuracy: 0.4991 - age_loss: 1.2985 - gender_accuracy: 0.8036 - gender_loss: 0.4289 - loss: 3.0696 - race_accuracy: 0.5142 - race_loss: 1.3423 - val_age_accuracy: 0.2458 - val_age_loss: 2.2065 - val_gender_accuracy: 0.6177 - val_gender_loss: 0.8164 - val_loss: 5.0971 - val_race_accuracy: 0.2588 - val_race_loss: 2.1249\n",
            "Epoch 8/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - age_accuracy: 0.5195 - age_loss: 1.2454 - gender_accuracy: 0.8164 - gender_loss: 0.4002 - loss: 2.9202 - race_accuracy: 0.5271 - race_loss: 1.2747\n",
            "Epoch 8: saving model to model_checkpoints/weights_epoch_08.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 157ms/step - age_accuracy: 0.5195 - age_loss: 1.2455 - gender_accuracy: 0.8165 - gender_loss: 0.4001 - loss: 2.9201 - race_accuracy: 0.5272 - race_loss: 1.2745 - val_age_accuracy: 0.2633 - val_age_loss: 2.1110 - val_gender_accuracy: 0.6255 - val_gender_loss: 0.8159 - val_loss: 4.9947 - val_race_accuracy: 0.2666 - val_race_loss: 2.1097\n",
            "Epoch 9/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - age_accuracy: 0.5440 - age_loss: 1.2015 - gender_accuracy: 0.8466 - gender_loss: 0.3632 - loss: 2.7924 - race_accuracy: 0.5504 - race_loss: 1.2277\n",
            "Epoch 9: saving model to model_checkpoints/weights_epoch_09.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 153ms/step - age_accuracy: 0.5441 - age_loss: 1.2015 - gender_accuracy: 0.8465 - gender_loss: 0.3632 - loss: 2.7922 - race_accuracy: 0.5506 - race_loss: 1.2275 - val_age_accuracy: 0.2731 - val_age_loss: 2.0357 - val_gender_accuracy: 0.6320 - val_gender_loss: 0.7904 - val_loss: 4.8827 - val_race_accuracy: 0.2757 - val_race_loss: 2.0898\n",
            "Epoch 10/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - age_accuracy: 0.5796 - age_loss: 1.1354 - gender_accuracy: 0.8596 - gender_loss: 0.3389 - loss: 2.6323 - race_accuracy: 0.5863 - race_loss: 1.1580\n",
            "Epoch 10: saving model to model_checkpoints/weights_epoch_10.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 152ms/step - age_accuracy: 0.5796 - age_loss: 1.1355 - gender_accuracy: 0.8595 - gender_loss: 0.3390 - loss: 2.6322 - race_accuracy: 0.5864 - race_loss: 1.1578 - val_age_accuracy: 0.2913 - val_age_loss: 1.9701 - val_gender_accuracy: 0.6326 - val_gender_loss: 0.7796 - val_loss: 4.7866 - val_race_accuracy: 0.2854 - val_race_loss: 2.0513\n",
            "Epoch 11/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - age_accuracy: 0.5973 - age_loss: 1.0867 - gender_accuracy: 0.8755 - gender_loss: 0.3128 - loss: 2.4918 - race_accuracy: 0.6182 - race_loss: 1.0923\n",
            "Epoch 11: saving model to model_checkpoints/weights_epoch_11.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 159ms/step - age_accuracy: 0.5973 - age_loss: 1.0867 - gender_accuracy: 0.8755 - gender_loss: 0.3128 - loss: 2.4915 - race_accuracy: 0.6184 - race_loss: 1.0920 - val_age_accuracy: 0.2939 - val_age_loss: 1.9445 - val_gender_accuracy: 0.6515 - val_gender_loss: 0.7385 - val_loss: 4.6818 - val_race_accuracy: 0.2926 - val_race_loss: 1.9962\n",
            "Epoch 12/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - age_accuracy: 0.6261 - age_loss: 1.0316 - gender_accuracy: 0.8865 - gender_loss: 0.2852 - loss: 2.3577 - race_accuracy: 0.6324 - race_loss: 1.0409\n",
            "Epoch 12: saving model to model_checkpoints/weights_epoch_12.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 157ms/step - age_accuracy: 0.6261 - age_loss: 1.0317 - gender_accuracy: 0.8864 - gender_loss: 0.2853 - loss: 2.3575 - race_accuracy: 0.6325 - race_loss: 1.0406 - val_age_accuracy: 0.3088 - val_age_loss: 1.8462 - val_gender_accuracy: 0.6612 - val_gender_loss: 0.7211 - val_loss: 4.5139 - val_race_accuracy: 0.3121 - val_race_loss: 1.9353\n",
            "Epoch 13/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - age_accuracy: 0.6416 - age_loss: 0.9891 - gender_accuracy: 0.8989 - gender_loss: 0.2660 - loss: 2.2518 - race_accuracy: 0.6475 - race_loss: 0.9967\n",
            "Epoch 13: saving model to model_checkpoints/weights_epoch_13.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 153ms/step - age_accuracy: 0.6417 - age_loss: 0.9891 - gender_accuracy: 0.8989 - gender_loss: 0.2661 - loss: 2.2515 - race_accuracy: 0.6477 - race_loss: 0.9964 - val_age_accuracy: 0.3108 - val_age_loss: 1.7664 - val_gender_accuracy: 0.6847 - val_gender_loss: 0.6812 - val_loss: 4.3548 - val_race_accuracy: 0.3270 - val_race_loss: 1.8889\n",
            "Epoch 14/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - age_accuracy: 0.6764 - age_loss: 0.9388 - gender_accuracy: 0.9137 - gender_loss: 0.2449 - loss: 2.1130 - race_accuracy: 0.6899 - race_loss: 0.9293\n",
            "Epoch 14: saving model to model_checkpoints/weights_epoch_14.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 157ms/step - age_accuracy: 0.6764 - age_loss: 0.9388 - gender_accuracy: 0.9136 - gender_loss: 0.2450 - loss: 2.1129 - race_accuracy: 0.6901 - race_loss: 0.9291 - val_age_accuracy: 0.3290 - val_age_loss: 1.7286 - val_gender_accuracy: 0.6879 - val_gender_loss: 0.6866 - val_loss: 4.3128 - val_race_accuracy: 0.3283 - val_race_loss: 1.8777\n",
            "Epoch 15/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - age_accuracy: 0.7059 - age_loss: 0.8803 - gender_accuracy: 0.9144 - gender_loss: 0.2414 - loss: 1.9932 - race_accuracy: 0.7077 - race_loss: 0.8714\n",
            "Epoch 15: saving model to model_checkpoints/weights_epoch_15.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 153ms/step - age_accuracy: 0.7058 - age_loss: 0.8805 - gender_accuracy: 0.9144 - gender_loss: 0.2414 - loss: 1.9931 - race_accuracy: 0.7078 - race_loss: 0.8713 - val_age_accuracy: 0.3355 - val_age_loss: 1.6934 - val_gender_accuracy: 0.7048 - val_gender_loss: 0.6403 - val_loss: 4.1946 - val_race_accuracy: 0.3427 - val_race_loss: 1.8430\n",
            "Epoch 16/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - age_accuracy: 0.7139 - age_loss: 0.8505 - gender_accuracy: 0.9224 - gender_loss: 0.2187 - loss: 1.8687 - race_accuracy: 0.7468 - race_loss: 0.7995\n",
            "Epoch 16: saving model to model_checkpoints/weights_epoch_16.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 152ms/step - age_accuracy: 0.7139 - age_loss: 0.8506 - gender_accuracy: 0.9224 - gender_loss: 0.2188 - loss: 1.8687 - race_accuracy: 0.7469 - race_loss: 0.7994 - val_age_accuracy: 0.3316 - val_age_loss: 1.6863 - val_gender_accuracy: 0.7146 - val_gender_loss: 0.6169 - val_loss: 4.1446 - val_race_accuracy: 0.3459 - val_race_loss: 1.8252\n",
            "Epoch 17/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - age_accuracy: 0.7399 - age_loss: 0.8011 - gender_accuracy: 0.9332 - gender_loss: 0.2068 - loss: 1.7730 - race_accuracy: 0.7575 - race_loss: 0.7650\n",
            "Epoch 17: saving model to model_checkpoints/weights_epoch_17.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 155ms/step - age_accuracy: 0.7398 - age_loss: 0.8012 - gender_accuracy: 0.9332 - gender_loss: 0.2068 - loss: 1.7729 - race_accuracy: 0.7576 - race_loss: 0.7649 - val_age_accuracy: 0.3381 - val_age_loss: 1.6731 - val_gender_accuracy: 0.7120 - val_gender_loss: 0.6255 - val_loss: 4.1033 - val_race_accuracy: 0.3576 - val_race_loss: 1.7881\n",
            "Epoch 18/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - age_accuracy: 0.7714 - age_loss: 0.7445 - gender_accuracy: 0.9412 - gender_loss: 0.1867 - loss: 1.6441 - race_accuracy: 0.7876 - race_loss: 0.7130\n",
            "Epoch 18: saving model to model_checkpoints/weights_epoch_18.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 150ms/step - age_accuracy: 0.7713 - age_loss: 0.7446 - gender_accuracy: 0.9413 - gender_loss: 0.1867 - loss: 1.6441 - race_accuracy: 0.7877 - race_loss: 0.7129 - val_age_accuracy: 0.3518 - val_age_loss: 1.6773 - val_gender_accuracy: 0.7146 - val_gender_loss: 0.6252 - val_loss: 4.0964 - val_race_accuracy: 0.3609 - val_race_loss: 1.7784\n",
            "Epoch 19/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - age_accuracy: 0.7709 - age_loss: 0.7296 - gender_accuracy: 0.9545 - gender_loss: 0.1665 - loss: 1.5725 - race_accuracy: 0.7893 - race_loss: 0.6764\n",
            "Epoch 19: saving model to model_checkpoints/weights_epoch_19.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 162ms/step - age_accuracy: 0.7709 - age_loss: 0.7296 - gender_accuracy: 0.9545 - gender_loss: 0.1665 - loss: 1.5723 - race_accuracy: 0.7895 - race_loss: 0.6762 - val_age_accuracy: 0.3472 - val_age_loss: 1.6900 - val_gender_accuracy: 0.7146 - val_gender_loss: 0.6265 - val_loss: 4.0849 - val_race_accuracy: 0.3758 - val_race_loss: 1.7543\n",
            "Epoch 20/20\n",
            "\u001b[1m192/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - age_accuracy: 0.7979 - age_loss: 0.6858 - gender_accuracy: 0.9530 - gender_loss: 0.1571 - loss: 1.4642 - race_accuracy: 0.8232 - race_loss: 0.6213\n",
            "Epoch 20: saving model to model_checkpoints/weights_epoch_20.weights.h5\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 153ms/step - age_accuracy: 0.7978 - age_loss: 0.6859 - gender_accuracy: 0.9531 - gender_loss: 0.1571 - loss: 1.4641 - race_accuracy: 0.8232 - race_loss: 0.6212 - val_age_accuracy: 0.3524 - val_age_loss: 1.6985 - val_gender_accuracy: 0.7191 - val_gender_loss: 0.6310 - val_loss: 4.0916 - val_race_accuracy: 0.3823 - val_race_loss: 1.7519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the last saved epoch log from the JSON file\n",
        "epoch_log_path = 'epoch_log.json'\n",
        "\n",
        "# Check if the log file exists and retrieve the last epoch details\n",
        "try:\n",
        "    with open(epoch_log_path, 'r') as file:\n",
        "        epoch_log = json.load(file)\n",
        "        last_epoch_values = epoch_log[-1]  # Get the last epoch details\n",
        "        last_epoch_values\n",
        "except FileNotFoundError:\n",
        "    last_epoch_values = \"No epoch log file found.\"\n",
        "except Exception as e:\n",
        "    last_epoch_values = f\"An error occurred: {e}\"\n",
        "\n",
        "last_epoch_values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eiyd5W8ZGihh",
        "outputId": "8fed19f3-2daa-458c-b9ea-88a966dba06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 20,\n",
              " 'age_accuracy': 0.7926154732704163,\n",
              " 'age_loss': 0.6943234205245972,\n",
              " 'gender_accuracy': 0.9547820687294006,\n",
              " 'gender_loss': 0.15857507288455963,\n",
              " 'loss': 1.4603650569915771,\n",
              " 'race_accuracy': 0.8262849450111389,\n",
              " 'race_loss': 0.6137818098068237,\n",
              " 'val_age_accuracy': 0.35240572690963745,\n",
              " 'val_age_loss': 1.6984646320343018,\n",
              " 'val_gender_accuracy': 0.7191157341003418,\n",
              " 'val_gender_loss': 0.6310276985168457,\n",
              " 'val_loss': 4.091618537902832,\n",
              " 'val_race_accuracy': 0.38231468200683594,\n",
              " 'val_race_loss': 1.7518839836120605}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Serialize model architecture and weights into a dictionary\n",
        "model_data = {\n",
        "    \"architecture\": model.to_json(),  # Save model architecture\n",
        "    \"weights\": model.get_weights(),  # Save model weights\n",
        "}\n",
        "\n",
        "# Save the model dictionary as a pickle file\n",
        "with open(\"model.pkl\", \"wb\") as file:\n",
        "    pickle.dump(model_data, file)\n",
        "\n",
        "print(\"Model saved as model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNP9mKYhTONC",
        "outputId": "541854df-c7c1-41a7-f829-eaeaf796aec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as model.pkl\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}